{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2a1180",
   "metadata": {},
   "source": [
    "# Extract Frame-level Features\n",
    "Convert MediaPipe Face Mesh landmark outputs into a frame-level feature table that can be used later for window-level aggregation and traditional machine learning models for drowsiness recognition.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Inputs\n",
    "- A root directory `data/` containing per-video landmark results:\n",
    "  - `*_landmarks.npz`\n",
    "      - NumPy archive with:\n",
    "          - `landmarks`: array of shape (T, 468, 3)\n",
    "              - T = number of processed frames\n",
    "              - 468 = MediaPipe Face Mesh points\n",
    "              - 3 = (x, y, z) normalized coordinates\n",
    "          - `frame_indices`: array of shape (T,) with original frame indices\n",
    "  - `*_meta.json`\n",
    "      - JSON metadata for each video, containing at least:\n",
    "          - `video_path`: original video path (not necessarily used as real path)\n",
    "          - `fps`, `total_frames_est`, `processed_frames`, etc.\n",
    "\n",
    "--\n",
    "\n",
    "## 2. Outputs\n",
    "- A CSV file with frame-level features, saved to:\n",
    "  - `exported_data/features_frame_level.csv`\n",
    "\n",
    "Each row in the CSV corresponds to one valid frame (where a face was detected), with the following columns:\n",
    "\n",
    "- `subject_id`: folder name containing the npz/json (e.g. \"01\")\n",
    "- `video_id`: npz file stem (e.g. \"0_landmarks\")\n",
    "- `frame_idx`: original frame index from `frame_indices`\n",
    "- `ear_left`: left eye aspect ratio (EAR)\n",
    "- `ear_right`: right eye aspect ratio (EAR)\n",
    "- `ear_mean`: mean of left/right EAR\n",
    "- `mar`: mouth aspect ratio (MAR)\n",
    "- `label`: placeholder for drowsiness label (alert / low / drowsy)\n",
    "          (currently None; to be filled later based on video grouping)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Main objectives of this notebook\n",
    "1. Recursively scans the given `data/` directory for all files matching `*_landmarks.npz`.\n",
    "\n",
    "2. For each `*_landmarks.npz`:\n",
    "   - Loads the `landmarks` array (T, 468, 3) and `frame_indices`.\n",
    "   - Loads the corresponding `*_meta.json`.\n",
    "   - Derives `subject_id` and `video_id` from the npz file path.\n",
    "   - For each frame:\n",
    "       - Skips frames where all landmarks are NaN (no face detected).\n",
    "       - Computes:\n",
    "           - eye aspect ratio (EAR) for left and right eye\n",
    "           - mouth aspect ratio (MAR)\n",
    "       - Stores these features together with `subject_id`, `video_id`, `frame_idx` and a placeholder `label`.\n",
    "\n",
    "3. Concatenates all per-video DataFrames into one large frame-level feature table using pandas.\n",
    "\n",
    "4. Creates the `exported_data/` directory if it does not exist and saves the final table as `features_frame_level.csv`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca514891",
   "metadata": {},
   "source": [
    "## Codes\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Import modules and database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215975e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['landmarks', 'frame_indices']\n",
      "(18053, 468, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "npz_path = \"data/01/0_landmarks.npz\"\n",
    "data = np.load(npz_path)\n",
    "print(data.files)   \n",
    "\n",
    "landmarks = data[data.files[0]]  \n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f4d7f",
   "metadata": {},
   "source": [
    "### 2. Define helpers for computation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e48a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index: a commonly used set of eye/mouth points in MediaPipe Face Mesh\n",
    "LEFT_EYE_IDX  = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE_IDX = [263, 385, 387, 362, 380, 373]\n",
    "MOUTH_IDX     = [13, 14, 78, 308, 82, 312]  # Upper/lower lip + left/right mouth corners\n",
    "\n",
    "# fonction: get the distance in 2D\n",
    "def dist2d(a, b):\n",
    "    return np.linalg.norm(a[:2] - b[:2])\n",
    "\n",
    "# fonction: compute EAR \n",
    "def eye_aspect_ratio(pts, idx):\n",
    "    # EAR formule： (‖p2-p6‖ + ‖p3-p5‖) / (2‖p1-p4‖ +1e-6)\n",
    "    p1, p2, p3, p4, p5, p6 = [pts[i] for i in idx]\n",
    "    vertical = dist2d(p2, p6) + dist2d(p3, p5)\n",
    "    horizontal = 2.0 * dist2d(p1, p4) + 1e-6\n",
    "    return vertical / horizontal\n",
    "\n",
    "# fonction：compute MAR\n",
    "def mouth_aspect_ratio(pts, idx):\n",
    "    # MAR formule： (‖p_up1 - p_down1‖ + ‖p_up2-p_down2‖) / (2‖p_left-p_right‖ +1e-6)\n",
    "    p_up1, p_down1, p_left, p_right, p_up2, p_down2 = [pts[i] for i in idx]\n",
    "    vertical = dist2d(p_up1, p_down1) + dist2d(p_up2, p_down2)\n",
    "    horizontal = 2.0 * dist2d(p_left, p_right) + 1e-6\n",
    "    return vertical / horizontal\n",
    "\n",
    "def compute_frame_features(pts):\n",
    "    \"\"\"\n",
    "    pts: (468, 3) numpy array for ONE frame\n",
    "    return: dict of features for this frame\n",
    "    \"\"\"\n",
    "    if np.isnan(pts).all():\n",
    "        return None  \n",
    "\n",
    "    ear_left = eye_aspect_ratio(pts, LEFT_EYE_IDX)\n",
    "    ear_right = eye_aspect_ratio(pts, RIGHT_EYE_IDX)\n",
    "    mar = mouth_aspect_ratio(pts, MOUTH_IDX)\n",
    "\n",
    "    return {\n",
    "        \"ear_left\": ear_left,\n",
    "        \"ear_right\": ear_right,\n",
    "        \"ear_mean\": (ear_left + ear_right) / 2.0,\n",
    "        \"mar\": mar,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a82375",
   "metadata": {},
   "source": [
    "### 3. Extract frame level features from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6efa25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id     video_id  frame_idx label  ear_left  ear_right  ear_mean  \\\n",
      "0         01  0_landmarks          1  None  0.150208   0.414041  0.282124   \n",
      "1         01  0_landmarks          2  None  0.129768   0.413464  0.271616   \n",
      "2         01  0_landmarks          3  None  0.130547   0.411373  0.270960   \n",
      "3         01  0_landmarks          4  None  0.126204   0.413346  0.269775   \n",
      "4         01  0_landmarks          5  None  0.128361   0.414641  0.271501   \n",
      "\n",
      "        mar  \n",
      "0  0.134435  \n",
      "1  0.133739  \n",
      "2  0.132525  \n",
      "3  0.133026  \n",
      "4  0.132456  \n",
      "Frames with features: 18049\n"
     ]
    }
   ],
   "source": [
    "def compute_features_for_video(npz_path, meta_path):\n",
    "    # read landmarks &  frame_indices\n",
    "    data = np.load(npz_path)\n",
    "    landmarks = data[\"landmarks\"]        # (T, 468, 3)\n",
    "    frame_indices = data[\"frame_indices\"]  # (T,)\n",
    "\n",
    "    # read meta.json\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    p = Path(npz_path)\n",
    "    subject_id = p.parent.name      # data/01/0_landmarks.npz -> \"01\"\n",
    "    video_id = p.stem               # \"0_landmarks\"\n",
    "\n",
    "    rows = []\n",
    "    T = landmarks.shape[0]\n",
    "\n",
    "    for i in range(T):\n",
    "        pts = landmarks[i]  # (468, 3)\n",
    "        feats = compute_frame_features(pts)\n",
    "        if feats is None:\n",
    "            continue  # skip the frame without face\n",
    "\n",
    "        row = {\n",
    "            \"subject_id\": subject_id,\n",
    "            \"video_id\": video_id,\n",
    "            \"frame_idx\": int(frame_indices[i]),\n",
    "            \"label\": meta.get(\"label\", None),  \n",
    "        }\n",
    "        row.update(feats)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "npz_path = \"data/01/0_landmarks.npz\"\n",
    "meta_path = \"data/01/0_meta.json\"\n",
    "\n",
    "df_one = compute_features_for_video(npz_path, meta_path)\n",
    "print(df_one.head())\n",
    "print(\"Frames with features:\", len(df_one))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391fdcd",
   "metadata": {},
   "source": [
    "### 4. Concatenates all per-video DataFrames into one large frame-level feature table `exported_data/features_frame_level.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a1a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data\\01\\0_landmarks.npz\n",
      "Processing: data\\01\\10_landmarks.npz\n",
      "Processing: data\\02\\0_landmarks.npz\n",
      "Processing: data\\02\\10_landmarks.npz\n",
      "Processing: data\\03\\0_landmarks.npz\n",
      "Processing: data\\03\\10_landmarks.npz\n",
      "Processing: data\\04\\0_landmarks.npz\n",
      "Processing: data\\04\\10_landmarks.npz\n",
      "Processing: data\\05\\0_landmarks.npz\n",
      "Processing: data\\05\\10_landmarks.npz\n",
      "Processing: data\\06\\0_landmarks.npz\n",
      "Processing: data\\06\\10_landmarks.npz\n",
      "Processing: data\\07\\0_landmarks.npz\n",
      "Processing: data\\07\\10_landmarks.npz\n",
      "Processing: data\\08\\0_landmarks.npz\n",
      "Processing: data\\08\\10_landmarks.npz\n",
      "Processing: data\\09\\0_landmarks.npz\n",
      "Processing: data\\09\\10_landmarks.npz\n",
      "Processing: data\\10\\0_landmarks.npz\n",
      "Processing: data\\10\\10_landmarks.npz\n",
      "Processing: data\\11\\0_landmarks.npz\n",
      "Processing: data\\11\\10_landmarks.npz\n",
      "Processing: data\\12\\0_landmarks.npz\n",
      "Processing: data\\12\\10_landmarks.npz\n",
      "Processing: data\\13\\0_landmarks.npz\n",
      "Processing: data\\13\\10_landmarks.npz\n",
      "Processing: data\\14\\0_landmarks.npz\n",
      "Processing: data\\14\\10_landmarks.npz\n",
      "Processing: data\\15\\0_landmarks.npz\n",
      "Processing: data\\15\\10_landmarks.npz\n",
      "Processing: data\\16\\0_landmarks.npz\n",
      "Processing: data\\16\\10_landmarks.npz\n",
      "Processing: data\\17\\0_landmarks.npz\n",
      "Processing: data\\17\\10_landmarks.npz\n",
      "Processing: data\\18\\0_landmarks.npz\n",
      "Processing: data\\18\\10_landmarks.npz\n",
      "Processing: data\\19\\0_landmarks.npz\n",
      "Processing: data\\19\\10_landmarks.npz\n",
      "Processing: data\\20\\0_landmarks.npz\n",
      "Processing: data\\20\\10_landmarks.npz\n",
      "Processing: data\\21\\0_landmarks.npz\n",
      "Processing: data\\21\\10_landmarks.npz\n",
      "Processing: data\\22\\0_landmarks.npz\n",
      "Processing: data\\22\\10_landmarks.npz\n",
      "Processing: data\\23\\0_landmarks.npz\n",
      "Processing: data\\23\\10_landmarks.npz\n",
      "Processing: data\\24\\0_landmarks.npz\n",
      "Processing: data\\24\\10_landmarks.npz\n",
      "Processing: data\\25\\0_landmarks.npz\n",
      "Processing: data\\25\\10_landmarks.npz\n",
      "Processing: data\\26\\0_landmarks.npz\n",
      "Processing: data\\26\\10_landmarks.npz\n",
      "Processing: data\\27\\0_landmarks.npz\n",
      "Processing: data\\27\\10_landmarks.npz\n",
      "Processing: data\\28\\0_landmarks.npz\n",
      "Processing: data\\28\\10_landmarks.npz\n",
      "Processing: data\\29\\0_landmarks.npz\n",
      "Processing: data\\29\\10_landmarks.npz\n",
      "Processing: data\\30\\0_landmarks.npz\n",
      "Processing: data\\30\\10_landmarks.npz\n",
      "Total frame-level samples: 1027076\n",
      "Saved to: exported_data\\features_frame_level.csv\n"
     ]
    }
   ],
   "source": [
    "root = Path(\"data\") \n",
    "all_dfs = []\n",
    "\n",
    "for npz_path in root.rglob(\"*_landmarks.npz\"):\n",
    "    meta_path = npz_path.with_name(npz_path.name.replace(\"_landmarks.npz\", \"_meta.json\"))\n",
    "    print(\"Processing:\", npz_path)\n",
    "    \n",
    "    df_video = compute_features_for_video(npz_path, meta_path)\n",
    "    all_dfs.append(df_video)\n",
    "\n",
    "df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"Total frame-level samples:\", len(df_all))\n",
    "\n",
    "# save as csv\n",
    "export_dir = \"exported_data\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(export_dir, \"features_frame_level.csv\")\n",
    "df_all.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Saved to:\", output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
